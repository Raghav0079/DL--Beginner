# Deep Learning for Beginners üß†

A comprehensive collection of Jupyter notebooks covering fundamental concepts in Deep Learning, designed for beginners who want to understand the core principles and implementations from scratch.

## üìö Repository Overview

This repository contains hands-on implementations and explanations of key deep learning concepts, algorithms, and techniques. Each notebook is designed to be educational and includes both theoretical explanations and practical code implementations.

## üóÇÔ∏è Content Structure

### Core Concepts

#### **Neural Network Fundamentals**
- [`perceptron.ipynb`](perceptron%20.ipynb) - Basic perceptron implementation and understanding
- [`perceptron-demo.ipynb`](perceptron-demo.ipynb) - Interactive perceptron demonstrations

#### **Backpropagation Algorithm**
- [`backpropogation - regression .ipynb`](backpropogation%20-%20regression%20.ipynb) - Backpropagation for regression problems
- [`backpropogation-classification.ipynb`](backpropogation-classification.ipynb) - Backpropagation for classification tasks

### Optimization Techniques

#### **Gradient Descent Variants**
- [`batch vs stochastic GD.ipynb`](batch%20vs%20stochastic%20GD.ipynb) - Comparison between batch and stochastic gradient descent

#### **Weight Initialization**
- [`xavier_he_initialization.ipynb`](xavier_he_initialization.ipynb) - Xavier and He initialization techniques
- [`zero-initialization-relu.ipynb`](zero-initialization-relu.ipynb) - Effects of zero initialization with ReLU activation
- [`zero-initialization-sigmoid.ipynb`](zero-initialization-sigmoid.ipynb) - Effects of zero initialization with Sigmoid activation

### Regularization and Overfitting

#### **Regularization Techniques**
- [`regularization.ipynb`](regularization.ipynb) - L1 and L2 regularization implementation
- [`dropout-regression.ipynb`](dropout-regression.ipynb) - Dropout implementation for regression
- [`dropout-classification.ipynb`](dropout-classification.ipynb) - Dropout implementation for classification
- [`early-stopping.ipynb`](early-stopping.ipynb) - Early stopping implementation to prevent overfitting

### Data Preprocessing
- [`feature-scaling.ipynb`](feature-scaling.ipynb) - Feature scaling techniques and their importance

### Common Problems and Solutions
- [`vanishing gradient.ipynb`](vanishing%20gradient.ipynb) - Understanding and solving vanishing gradient problem

### Practical Applications

#### **Real-world Projects**
- [`customer churn prediction.ipynb`](customer%20churn%20prediction.ipynb) - Customer churn prediction using neural networks
- [`graduate admission regression .ipynb`](graduate%20admission%20regression%20.ipynb) - Graduate admission prediction
- [`mnist-classification.ipynb.ipynb`](mnist-classification.ipynb.ipynb) - MNIST digit classification

#### **Datasets**
- [`placement.csv`](placement.csv) - Dataset for placement prediction tasks

## üöÄ Getting Started

### Prerequisites
```bash
pip install numpy pandas matplotlib scikit-learn tensorflow keras jupyter
```

### Running the Notebooks
1. Clone this repository
```bash
git clone https://github.com/Raghav0079/DL--Beginner.git
cd DL--Beginner
```

2. Launch Jupyter Notebook
```bash
jupyter notebook
```

3. Open any notebook and start learning!

## üìñ Learning Path

For beginners, we recommend following this learning sequence:

### **Level 1: Fundamentals**
1. [Perceptron](perceptron%20.ipynb) - Start here to understand the basic building block
2. [Perceptron Demo](perceptron-demo.ipynb) - Interactive demonstrations
3. [Feature Scaling](feature-scaling.ipynb) - Learn about data preprocessing

### **Level 2: Core Algorithms**
1. [Backpropagation - Regression](backpropogation%20-%20regression%20.ipynb)
2. [Backpropagation - Classification](backpropogation-classification.ipynb)
3. [Batch vs Stochastic GD](batch%20vs%20stochastic%20GD.ipynb)

### **Level 3: Optimization and Initialization**
1. [Xavier/He Initialization](xavier_he_initialization.ipynb)
2. [Zero Initialization Effects](zero-initialization-relu.ipynb)
3. [Vanishing Gradient Problem](vanishing%20gradient.ipynb)

### **Level 4: Regularization**
1. [Regularization Techniques](regularization.ipynb)
2. [Dropout Implementation](dropout-regression.ipynb)
3. [Early Stopping](early-stopping.ipynb)

### **Level 5: Real-world Applications**
1. [MNIST Classification](mnist-classification.ipynb.ipynb)
2. [Customer Churn Prediction](customer%20churn%20prediction.ipynb)
3. [Graduate Admission Prediction](graduate%20admission%20regression%20.ipynb)

## üéØ Key Learning Outcomes

After completing this repository, you will understand:

- **Neural Network Basics**: How perceptrons work and form the foundation of neural networks
- **Backpropagation**: The core algorithm that makes neural network learning possible
- **Optimization**: Different gradient descent variants and their trade-offs
- **Regularization**: Techniques to prevent overfitting and improve generalization
- **Weight Initialization**: Why proper initialization matters and how to do it right
- **Common Problems**: How to identify and solve issues like vanishing gradients
- **Practical Implementation**: Real-world applications and best practices

## üõ†Ô∏è Technologies Used

- **Python**: Primary programming language
- **NumPy**: Numerical computations
- **Pandas**: Data manipulation and analysis
- **Matplotlib**: Data visualization
- **Scikit-learn**: Machine learning utilities
- **TensorFlow/Keras**: Deep learning frameworks
- **Jupyter Notebook**: Interactive development environment

## üìä Datasets

The repository includes various datasets for practical learning:
- Placement prediction data
- Customer data for churn analysis
- Graduate admission data
- MNIST digit dataset (loaded automatically)

## ü§ù Contributing

Contributions are welcome! If you have suggestions for improvements or additional notebooks that would benefit beginners, please:

1. Fork the repository
2. Create a feature branch
3. Add your improvements
4. Submit a pull request

## üìù License

This project is open source and available under the [MIT License](LICENSE).

## üôè Acknowledgments

- Thanks to the open-source community for providing excellent learning resources
- Special thanks to educators and researchers who make deep learning accessible to beginners

## üìû Contact

**Raghav**
- GitHub: [@Raghav0079](https://github.com/Raghav0079)

---

‚≠ê **Star this repository if you find it helpful!** ‚≠ê

Happy Learning! üöÄ